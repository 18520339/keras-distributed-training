{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T02:21:02.256777Z",
     "iopub.status.busy": "2022-01-14T02:21:02.256117Z",
     "iopub.status.idle": "2022-01-14T02:21:04.257780Z",
     "shell.execute_reply": "2022-01-14T02:21:04.257168Z"
    },
    "id": "vHNvttzV43sA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-worker configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU') \n",
    "if len(gpu_devices) == 0: raise SystemError('GPU device not found')\n",
    "for gpu in gpu_devices: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config = {\n",
    "    'cluster': {\n",
    "        'worker': ['192.168.1.1:12345', '192.168.1.2:12345']\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "}\n",
    "os.environ.pop('TF_CONFIG', None)\n",
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:GPU:0']\n",
      "INFO:tensorflow:Waiting for the cluster, timeout = inf\n",
      "INFO:tensorflow:Cluster is ready.\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['192.168.1.1:12345', '192.168.1.2:12345']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CommunicationImplementation.RING\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "    communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "        implementation = tf.distribute.experimental.CollectiveCommunication.RING\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "PER_WORKER_BATCH_SIZE = 32\n",
    "NUM_WORKERS = len(tf_config['cluster']['worker'])\n",
    "GLOBAL_BATCH_SIZE = PER_WORKER_BATCH_SIZE * NUM_WORKERS\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling\n",
    "data_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "data_root = tf.keras.utils.get_file('flower_photos', data_url, untar=True)\n",
    "data_length = !find {data_root} -name *.jpg | wc -l\n",
    "data_length = int(data_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(batch_size, subset):\n",
    "    shuffle = False\n",
    "    if subset == 'validation': length = int(data_length * 0.1)\n",
    "    elif subset == 'training': \n",
    "        length = int(data_length * 0.9)\n",
    "        shuffle = True\n",
    "    else: \n",
    "        raise NameError(\"subset must be 'training' or 'validation'\")\n",
    "        \n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "      str(data_root),\n",
    "      validation_split = 0.1,\n",
    "      subset = subset,\n",
    "      image_size = IMAGE_SIZE,\n",
    "      batch_size = batch_size,\n",
    "      seed = 123,\n",
    "    )\n",
    "    normalization_layer = Rescaling(1./127.5, offset=-1)\n",
    "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "    \n",
    "    if shuffle: dataset = dataset.shuffle(buffer_size=length)\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 3303 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 367 files for validation.\n"
     ]
    }
   ],
   "source": [
    "ds_train, num_train = get_dataset(GLOBAL_BATCH_SIZE, 'training')\n",
    "ds_val, num_val = get_dataset(GLOBAL_BATCH_SIZE, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model():\n",
    "    feature_extractor_layer = hub.KerasLayer(\n",
    "        'https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4',\n",
    "        input_shape = IMAGE_SIZE + (3,),\n",
    "        trainable = True\n",
    "    )\n",
    "    model = Sequential([feature_extractor_layer, Dense(5)])\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1001)              3540265   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,545,275\n",
      "Trainable params: 3,511,163\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "    model = build_and_compile_model()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epoch):\n",
    "    if epoch < 3: return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7: return 1e-4\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback for printing the learning rate at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'\\nLearning rate for epoch {epoch + 1} is {model.optimizer.lr.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler\n",
    "callbacks = [\n",
    "    TensorBoard(log_dir='./logs'),\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "    LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]\n",
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 160 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 160 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 160 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 160 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - ETA: 0s - loss: 0.8634 - accuracy: 0.7926INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = RING, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "52/52 [==============================] - 32s 435ms/step - loss: 0.8634 - accuracy: 0.7926 - val_loss: 7.0352 - val_accuracy: 0.7003 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.6502 - accuracy: 0.8710\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "52/52 [==============================] - 19s 374ms/step - loss: 0.6502 - accuracy: 0.8710 - val_loss: 31.4444 - val_accuracy: 0.3188 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.8968\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "52/52 [==============================] - 18s 351ms/step - loss: 0.4739 - accuracy: 0.8968 - val_loss: 0.7296 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9555\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "52/52 [==============================] - 18s 347ms/step - loss: 0.2622 - accuracy: 0.9555 - val_loss: 0.4127 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9794\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "52/52 [==============================] - 18s 349ms/step - loss: 0.1890 - accuracy: 0.9794 - val_loss: 0.3443 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9900\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "52/52 [==============================] - 18s 344ms/step - loss: 0.1613 - accuracy: 0.9900 - val_loss: 0.3247 - val_accuracy: 0.9428 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9967\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "52/52 [==============================] - 18s 346ms/step - loss: 0.1471 - accuracy: 0.9967 - val_loss: 0.3224 - val_accuracy: 0.9401 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9988\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "52/52 [==============================] - 18s 348ms/step - loss: 0.1412 - accuracy: 0.9988 - val_loss: 0.3276 - val_accuracy: 0.9428 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9988\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.1401 - accuracy: 0.9988 - val_loss: 0.3321 - val_accuracy: 0.9455 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9988\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "52/52 [==============================] - 18s 350ms/step - loss: 0.1400 - accuracy: 0.9988 - val_loss: 0.3356 - val_accuracy: 0.9482 - lr: 1.0000e-05\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 28580), started 0:18:32 ago. (Use '!kill 28580' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91c959933b97a5d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91c959933b97a5d7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    ds_train, \n",
    "    validation_data = ds_val,\n",
    "    callbacks = callbacks,\n",
    "    epochs = EPOCHS, \n",
    "    verbose = 1,\n",
    ")\n",
    "%tensorboard --logdir=logs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "multi_worker_with_keras.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
